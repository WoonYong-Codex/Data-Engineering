{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS103 - Data Engineering - Final Project \n",
    "### Name: Wong Woon Yong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "‘Web Scraping’ is a technique for gathering structured data or information from web pages, a quick way to acquire data which is presented on the web with a particular format. In this Web-Scraping project which will show the 3 basic step of retrieving information from web-page,<br> \n",
    "\n",
    "Step 1: Accessing the target Website using HTTP library requests.<br>\n",
    "Step 2: Parse the content of web using Web Parsing library Beautiful Soup.<br>\n",
    "Step 3: Save result to DataFrame format.<br>\n",
    "\n",
    "\n",
    "### Objective\n",
    "From the NEWS website shows the process of Parsing and Collecting information, by using \"Requests\" library to make an HTTP request and collect the HTML. Check for the connection information and scrape the HTML with using \"Beautiful Soup\". Target to retrieve the Title, Header (H1 and H2) and HTML elements on the page by the tag name (2 tag name). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and modules for selecting HTML tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup, diagnose\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Established a Connection to a News Web-page.\n",
    "Start with a Connection setup to Straits times websites and Connection Status was checked with Reponses 200. The \"headers\" in the code indicate connection setup as \"User\".\n",
    "\n",
    "* Code Details:<br>\n",
    "1) Code 200 series indicating connection successful <br>\n",
    "2) Code 400 series is \"Forbidden, cannot access due to blocked or protected\" <br>\n",
    "3) Code 500 series indicating \"Server Error\" <br>\n",
    "\n",
    "* Important: Only when having connection status with Code 200 then can proceed to next step.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.straitstimes.com/global\"      # Assign Web-page link to \"url\"\n",
    "connection1 = requests.get(url, headers={\"user-agent\":\"Mozilla/80.0\"})\n",
    "\n",
    "connection1.status_code    # Check the Connection status\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content check from web-page\n",
    "With the successful on connection of Code 200, the content on the web-page can be check by using a direct \".content\" function. But this method will display all content without proper format and is very confusing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection1.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Parsing the content and extracting the text\n",
    "### a) Applying Parser\n",
    "\n",
    "Generally, it is common to use BeautifulSoup in conjunction with the requests library, by applying the requests that fetch a page and BeautifulSoup will extract the resulting data or parsing the HTML. This is because data from the web-page itself are raw and need to parse for further understand on the content.\n",
    "\n",
    "There are few types of Parser, here showing example of (a) \"html.parser\" and (b) \"html5lib\", and assigning to a variable \"soup\". The result will then be print with \".prettify()\" function that will make the HTML code look better.\n",
    "\n",
    "\n",
    "### i) html.parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(connection1.content, 'html.parser') \n",
    "print(soup.prettify()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_2 = BeautifulSoup(connection1.content,'html5lib')\n",
    "print(soup_2.prettify()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation on the two Parser method above\n",
    "- The result of using both 'html.parser' and 'html5lib' parser is not much different, all contents that extracted from the web-page are 95% similar. \n",
    "- Based on research, the 'html.parser' speed is decent and the \"html5lib\" is slower, but the \"html5lib\" is much better in handling tangling tag issues that somehow happen in the middle of the web-page. For example, a Paragraph (tag-p) without closing issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *** Adding a Check on available Parser\n",
    "By applying a simple check with the \"diagnose\" function from bs4, the available parser can be indentify, and also indication on which parser can give the best result. <br>\n",
    "\n",
    "### i. Create a dummy of \"copy_html\" and check with \"diagnose\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_html = \"\"\" \n",
    "<!DOCTYPE html>\n",
    "<!--[if IE 8]> <html class=\"no-js lt-ie9 is-ie\"> <![endif]-->\n",
    "<!--[if IE 9]> <html class=\"no-js is-ie\"> <![endif]-->\n",
    "<!--[if gt IE 9]><!-->\n",
    "<html class=\"no-js\" dir=\"ltr\" lang=\"en\" prefix=\"og: http://ogp.me/ns# article: http://ogp.me/ns/article# book: http://ogp.me/ns/book# profile: http://ogp.me/ns/profile# video: http://ogp.me/ns/video# product: http://ogp.me/ns/product# content: http://purl.org/rss/1.0/modules/content/ dc: http://purl.org/dc/terms/ foaf: http://xmlns.com/foaf/0.1/ rdfs: http://www.w3.org/2000/01/rdf-schema# sioc: http://rdfs.org/sioc/ns# sioct: http://rdfs.org/sioc/types# skos: http://www.w3.org/2004/02/skos/core# xsd: http://www.w3.org/2001/XMLSchema#\">\n",
    " <!--<![endif]-->\n",
    " <head profile=\"http://www.w3.org/1999/xhtml/vocab\">\n",
    "  <meta charset=\"utf-8\"/>\n",
    "  <meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n",
    "  <meta content=\"width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no\" name=\"viewport\"/>\n",
    "  <script src=\"/sites/all/themes/custom/bootdemo/js/ads_checker.js\">\n",
    "  </script>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Check with \"diagnose\"\n",
    "Apply the \"diagnose\" function on the dummy of \"copy_html\" and check for the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnostic running on Beautiful Soup 4.9.1\n",
      "Python version 3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]\n",
      "Found lxml version 4.5.2.0\n",
      "Found html5lib version 1.1\n",
      "\n",
      "Trying to parse your markup with html.parser\n",
      "Here's what html.parser did with the markup:\n",
      "<!DOCTYPE html>\n",
      "<!--[if IE 8]> <html class=\"no-js lt-ie9 is-ie\"> <![endif]-->\n",
      "<!--[if IE 9]> <html class=\"no-js is-ie\"> <![endif]-->\n",
      "<!--[if gt IE 9]><!-->\n",
      "<html class=\"no-js\" dir=\"ltr\" lang=\"en\" prefix=\"og: http://ogp.me/ns# article: http://ogp.me/ns/article# book: http://ogp.me/ns/book# profile: http://ogp.me/ns/profile# video: http://ogp.me/ns/video# product: http://ogp.me/ns/product# content: http://purl.org/rss/1.0/modules/content/ dc: http://purl.org/dc/terms/ foaf: http://xmlns.com/foaf/0.1/ rdfs: http://www.w3.org/2000/01/rdf-schema# sioc: http://rdfs.org/sioc/ns# sioct: http://rdfs.org/sioc/types# skos: http://www.w3.org/2004/02/skos/core# xsd: http://www.w3.org/2001/XMLSchema#\">\n",
      " <!--<![endif]-->\n",
      " <head profile=\"http://www.w3.org/1999/xhtml/vocab\">\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no\" name=\"viewport\"/>\n",
      "  <script src=\"/sites/all/themes/custom/bootdemo/js/ads_checker.js\">\n",
      "  </script>\n",
      " </head>\n",
      "</html>\n",
      "--------------------------------------------------------------------------------\n",
      "Trying to parse your markup with html5lib\n",
      "Here's what html5lib did with the markup:\n",
      "<!DOCTYPE html>\n",
      "<!--[if IE 8]> <html class=\"no-js lt-ie9 is-ie\"> <![endif]-->\n",
      "<!--[if IE 9]> <html class=\"no-js is-ie\"> <![endif]-->\n",
      "<!--[if gt IE 9]><!-->\n",
      "<html class=\"no-js\" dir=\"ltr\" lang=\"en\" prefix=\"og: http://ogp.me/ns# article: http://ogp.me/ns/article# book: http://ogp.me/ns/book# profile: http://ogp.me/ns/profile# video: http://ogp.me/ns/video# product: http://ogp.me/ns/product# content: http://purl.org/rss/1.0/modules/content/ dc: http://purl.org/dc/terms/ foaf: http://xmlns.com/foaf/0.1/ rdfs: http://www.w3.org/2000/01/rdf-schema# sioc: http://rdfs.org/sioc/ns# sioct: http://rdfs.org/sioc/types# skos: http://www.w3.org/2004/02/skos/core# xsd: http://www.w3.org/2001/XMLSchema#\">\n",
      " <!--<![endif]-->\n",
      " <head profile=\"http://www.w3.org/1999/xhtml/vocab\">\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no\" name=\"viewport\"/>\n",
      "  <script src=\"/sites/all/themes/custom/bootdemo/js/ads_checker.js\">\n",
      "  </script>\n",
      " </head>\n",
      " <body>\n",
      " </body>\n",
      "</html>\n",
      "--------------------------------------------------------------------------------\n",
      "Trying to parse your markup with lxml\n",
      "Here's what lxml did with the markup:\n",
      "<!DOCTYPE html>\n",
      "<!--[if IE 8]> <html class=\"no-js lt-ie9 is-ie\"> <![endif]-->\n",
      "<!--[if IE 9]> <html class=\"no-js is-ie\"> <![endif]-->\n",
      "<!--[if gt IE 9]><!-->\n",
      "<html class=\"no-js\" dir=\"ltr\" lang=\"en\" prefix=\"og: http://ogp.me/ns# article: http://ogp.me/ns/article# book: http://ogp.me/ns/book# profile: http://ogp.me/ns/profile# video: http://ogp.me/ns/video# product: http://ogp.me/ns/product# content: http://purl.org/rss/1.0/modules/content/ dc: http://purl.org/dc/terms/ foaf: http://xmlns.com/foaf/0.1/ rdfs: http://www.w3.org/2000/01/rdf-schema# sioc: http://rdfs.org/sioc/ns# sioct: http://rdfs.org/sioc/types# skos: http://www.w3.org/2004/02/skos/core# xsd: http://www.w3.org/2001/XMLSchema#\">\n",
      " <!--<![endif]-->\n",
      " <head profile=\"http://www.w3.org/1999/xhtml/vocab\">\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no\" name=\"viewport\"/>\n",
      "  <script src=\"/sites/all/themes/custom/bootdemo/js/ads_checker.js\">\n",
      "  </script>\n",
      " </head>\n",
      "</html>\n",
      "--------------------------------------------------------------------------------\n",
      "Trying to parse your markup with lxml-xml\n",
      "Here's what lxml-xml did with the markup:\n",
      "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
      "<!DOCTYPE html>\n",
      "<!--[if IE 8]> <html class=\"no-js lt-ie9 is-ie\"> <![endif]-->\n",
      "<!--[if IE 9]> <html class=\"no-js is-ie\"> <![endif]-->\n",
      "<!--[if gt IE 9]><!-->\n",
      "<html class=\"no-js\" dir=\"ltr\" lang=\"en\" prefix=\"og: http://ogp.me/ns# article: http://ogp.me/ns/article# book: http://ogp.me/ns/book# profile: http://ogp.me/ns/profile# video: http://ogp.me/ns/video# product: http://ogp.me/ns/product# content: http://purl.org/rss/1.0/modules/content/ dc: http://purl.org/dc/terms/ foaf: http://xmlns.com/foaf/0.1/ rdfs: http://www.w3.org/2000/01/rdf-schema# sioc: http://rdfs.org/sioc/ns# sioct: http://rdfs.org/sioc/types# skos: http://www.w3.org/2004/02/skos/core# xsd: http://www.w3.org/2001/XMLSchema#\">\n",
      " <!--<![endif]-->\n",
      " <head profile=\"http://www.w3.org/1999/xhtml/vocab\">\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no\" name=\"viewport\"/>\n",
      "  <script src=\"/sites/all/themes/custom/bootdemo/js/ads_checker.js\">\n",
      "  </script>\n",
      " </head>\n",
      "</html>\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "diagnose.diagnose(copy_html)       # applying diagnose model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obsevation on Diagnose Result:\n",
    "The \"html5lib\" is the best parser in handling missing tag, from the retrieved data above, we can see it replaced the missing tag of **\"< body >    < /body >\"** accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Parsing the content and extracting the text\n",
    "### b) Extracting Web-page Title\n",
    "HTML containing different type of tags, it is necessary to understand what each html tags stand for, and that's will help to identify the tag pattern and ease the web scraping process. First, extracting the Title element, it is a required HTML element that used to assign a title to an HTML document.<br>\n",
    "\n",
    "*** The title can be directly extract by using simple \".title\" method, or using a more comprehensive  of \".find_all()\" function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>The Straits Times - Breaking News, Lifestyle &amp; Multimedia News</title>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title                           # Simple \".title\" method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All title in the web-page: \n",
      " [<title>The Straits Times - Breaking News, Lifestyle &amp; Multimedia News</title>]\n"
     ]
    }
   ],
   "source": [
    "title = soup.find_all('title')       # Extracting the \"title\" with \".find_all\" function.\n",
    "print(\"All title in the web-page: \\n\", title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Headlines and Sub-headlines.\n",
    "In a web-page, header tags have their own place and to be used in a proper order start with header or h1. It contains targeted keywords and close to related page title and content. Where sub-header or h2 should contain similar keywords as h1 tag.\n",
    "Here applying the \".find_all()\" function to extract the header and sub-header.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total links in my website : 14\n",
      "<h1 class=\"site-name\"><a class=\"name navbar-brand\" href=\"/\" title=\"Home\"><span>The Straits Times</span></a></h1>\n",
      "<h2 class=\"pane-title\">\n",
      "            Top Stories          </h2>\n",
      "<h2 class=\"pane-title\">\n",
      "            top picks          </h2>\n",
      "<h2 class=\"pane-title\">\n",
      "            covid-19          </h2>\n",
      "<h2 class=\"pane-title\">\n",
      "            For Subscribers          </h2>\n",
      "<h2 class=\"pane-title\">\n",
      "            VIEWS          </h2>\n",
      "<h2 class=\"pane-title\">\n",
      "            Asian Insider          </h2>\n",
      "<h2 class=\"pane-title\">\n",
      "            DISCOVER          </h2>\n",
      "<h2 class=\"pane-title\">\n",
      "            Videos          </h2>\n",
      "<h2 class=\"pane-title\">\n",
      "            GZERO MEDIA          </h2>\n",
      "<h2 class=\"pane-title\">\n",
      "            PODCASTS          </h2>\n",
      "<h2 class=\"pane-title\">\n",
      "            MULTIMEDIA          </h2>\n",
      "<h2 class=\"pane-title\">\n",
      "            MOST POPULAR          </h2>\n",
      "<h2 class=\"pane-title\">\n",
      "            Branded Content          </h2>\n"
     ]
    }
   ],
   "source": [
    "header_chk=soup.find_all(['h1','h2'])       # Applying \"find_all\" for h1 and h2.\n",
    "total_links=len(header_chk)                 # count the total number of h1 and h2 in the web-page.\n",
    "print(\"total links in my website :\", total_links)\n",
    "\n",
    "for a in header_chk:       # Using for-loop to display the h1 and h2.\n",
    "    print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting other HTML tags - \"a\" and \"span\"\n",
    "Next, extracting other type of tags and similarly with applying the \".find_all()\" function to locate the following two tags,<br>\n",
    "\n",
    "1) An anchor or \"a\" element is used to create hyperlink for a web-page or a location within the web-page itself. <br>\n",
    "2) Span element which used to select inline content for purely styling purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total links in my website : 215\n",
      "<a class=\"element-invisible element-focusable\" href=\"#main-content\">Skip to main content</a>\n",
      "<a class=\"name navbar-brand\" href=\"/\" title=\"Home\"><span>The Straits Times</span></a>\n",
      "<a class=\"name navbar-brand\" href=\"/\" title=\"Home\"><span>The Straits Times</span></a>\n",
      "<a class=\"globallink-ed\" href=\"/global/\" id=\"global-ed\">International</a>\n",
      "<a class=\"sinlink-ed\" href=\"/\" id=\"sin-ed\">Singapore</a>\n",
      "<a href=\"http://stepaper.straitstimes.com\" target=\"_blank\">E-paper</a>\n",
      "<a href=\"/\">Home</a>\n",
      "<a href=\"/singapore\">Singapore</a>\n"
     ]
    }
   ],
   "source": [
    "other_tag1=soup.find_all('a')\n",
    "total_links=len(other_tag1) \n",
    "print(\"total links in my website :\", total_links)\n",
    "\n",
    "for i in other_tag1[:8]:        # print 8 rows for checking\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total SPAN tag in this website (displaying 8 rows):  97\n",
      "<span class=\"sr-only\">Toggle navigation</span>\n",
      "<span class=\"icon-bar\"></span>\n",
      "<span class=\"icon-bar\"></span>\n",
      "<span class=\"icon-bar\"></span>\n",
      "<span>The Straits Times</span>\n",
      "<span>The Straits Times</span>\n",
      "<span class=\"sr-only\">Toggle navigation</span>\n",
      "<span class=\"icon-bar\"></span>\n"
     ]
    }
   ],
   "source": [
    "other_tag2=soup.find_all('span')\n",
    "total_links=len(other_tag2) \n",
    "print(\"\\nTotal SPAN tag in this website (displaying 8 rows): \", total_links)\n",
    "\n",
    "for i in other_tag2[:8]:       # print 8 rows for checking\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Extract - removing the tag name\n",
    "In order to remove the tags name from the retrieved information, printing the result with using \".get_text()\" function.\n",
    "Example of using the span tag results and after running the \".get_text()\" function, the result (in cell below) is more easy to read and understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total SPAN tag in this website (displaying 8 rows):  97\n",
      "Toggle navigation\n",
      "\n",
      "\n",
      "\n",
      "The Straits Times\n",
      "The Straits Times\n",
      "Toggle navigation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "other_tag2=soup.find_all('span')\n",
    "total_links=len(other_tag2) \n",
    "print(\"\\nTotal SPAN tag in this website (displaying 8 rows): \", total_links)\n",
    "\n",
    "for i in other_tag2[:8]:         # print 8 rows for checking\n",
    "    print(i.get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatively, the tag name can be remove by using \".text.strip()\" function\n",
    "Below using a for-loop and \".text.strip()\" on the \"a-tag\". After remove the tag name and save the information into an \"a_list\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Skip to main content', 'The Straits Times', 'The Straits Times', 'International', 'Singapore', 'E-paper', 'Home', 'Singapore', 'Jobs', 'Housing', 'Parenting & Education', 'Politics', 'Health', 'Transport', 'Courts & Crime', 'Consumer', 'Environment', 'Community', 'Asia', 'SE Asia', 'East Asia', 'South Asia', 'Australia/NZ', 'World', 'United States', 'Europe', 'Middle East', 'Opinion', 'ST Editorial', 'Cartoons', 'Forum', 'Life', 'Food', 'Entertainment', 'Style', 'Travel', 'Arts', 'Motoring', 'Home & Design', 'Business', 'Economy', 'Invest', 'Banking', 'Companies & Markets', 'Property', 'Tech', 'Tech News', 'E-sports', 'Reviews', 'Sport', 'Football', 'Schools', 'Formula One', 'Combat Sports', 'Basketball', 'Tennis', 'Golf', 'More', 'Opinion', 'Life', 'Business', 'Tech', 'Sport', 'Videos', 'Podcasts', 'Multimedia', 'SPH Websites', 'news with benefits', 'SPH Rewards', 'STJobs', 'STCars', 'STProperty', 'STClassifieds', 'SITES', 'Berita Harian', 'Hardwarezone', 'Lianhe Wanbao', 'STOMP', 'SGCarMart', 'SRX Property', 'tabla', 'Tamil Murasu', 'The Business Times', 'The New Paper', 'zaobao.sg', 'Obits.sg', 'Advertise with us', '', 'Trump becomes first US president to be impeached twice', 'Trump denounces political violence after being impeached', 'What will Congress do now that Trump has been impeached a second time?', \"Over 6,200 in S'pore received Covid-19 vaccine so far; 4 vaccination centres by end-Jan\", 'Japan expands Covid-19 emergency beyond Tokyo to cover Osaka, Nagoya and Fukuoka', 'South Korean prosecutors charge woman who abused 16-month-old daughter to death with murder', 'Quieter streets and supermarkets in Malaysia as Covid-19 movement controls start to kick in', 'Cancelled trip to Taiwan by US envoy unlikely to affect US-Taiwan relations', 'More headlines', 'E-paper', 'Facebook', 'Instagram', 'Twitter', 'Podcasts', 'Sign up', 'newsletters', '3 in 4 Covid-19 patients suffer from at least one symptom six months after infection: Study', 'Riding the wave of a travel drought: Yacht rentals, water sports gain popularity', \"To vaccinate or not: Calculations are different if you think from the country's perspective\", 'Covid-19 vaccine 101: All you need to know, from what to expect to how safe it is', 'More', 'China reports 138 new Covid-19 cases, highest number in more than 10 months', \"US doctor's death after Covid-19 vaccination is being investigated\", \"S'pore will review Sinovac Covid-19 vaccine carefully before possible roll-out: Gan Kim Yong\", 'After Covid-19 infection health workers 83% protected: British study', 'MORE', 'Cancelled trip to Taiwan by US envoy unlikely to affect US-Taiwan relations', 'Chance for US-China reset in Biden era', 'Time to wake up from techno-utopian dreams?', 'Why the world needs nuclear Deterrence 3.0', \"Fish and vegetable prices up at wet markets in S'pore due to floods in Malaysia\", 'Market outlook for year appears positive, but expect volatility', '', 'Read PDF', \"Football: Five factors behind Manchester United's rise to the Premier League's summit\", 'Emily Blunt is woman chasing after her dream guy in Wild Mountain Thyme', 'Subscribe', 'More Premium Stories', 'Ravi Velloor', 'Qatar peace deal and the lessons for Asia', '', 'Nirmal Ghosh', \"US Capitol riot a close call, sparks security concern ahead of Biden's inauguration\", '', 'Leslie Lopez', \"Emergency adds uncertainty to Malaysia's outlook\", '', 'Markus Ziener', \"Who will take up Merkel's mantle?\", '', 'More', 'Questions abound in Malaysia on whether new Covid-19 lockdown came too late', 'Asian Insider, Jan 13: Sinovac vaccine’s efficacy; America’s Indo-Pacific plans; China on Hong Kong', 'Japan extends state of emergency to seven more prefectures', 'Cancelled trip to Taiwan by US envoy unlikely to affect US-Taiwan relations', \"Jokowi gets Covid-19 jab as Indonesia's vaccination drive kicks off\", 'ST Asian Insider magazine: Download Jan issue here', \"South Korea's social atmosphere discourages childbirth: Korea Herald columnist\", \"China's rural areas require better Covid-19 defences: China Daily\", 'MORE', 'The news in cartoons by ST artists: Dec 2020, Jan 2021', 'From TraceTogether to LeaveHomeSafe: Covid-19 contact tracing apps around the world', 'Storming of the US Capitol: How the chaos unfolded', 'In Pictures: Trump supporters storm US Capitol', 'What is causing all the rain?', 'Covid-19: One year on, the fight continues', 'What happened to the KL-Singapore HSR project', 'Interactive: 20 words that define 2020', 'Life power list: What rocked the lifestyle scene here in 2020', 'Hotter, wetter, fiercer: A year of wild weather', 'The race for a Covid-19 vaccine', 'The long, cold journey Covid-19 vaccines could take to Singapore', 'MORE', '', 'More', 'GZERO VIDEO: Trumpism will continue to have a political space in America', 'GZERO VIDEO: London will not be seen as much of a global city after Brexit, says Bremmer', 'GZERO VIDEO: How is Trump trying to stay relevant?', 'MORE', 'Addressing Covid-19 vaccine safety with Prof Ooi Eng Eong', \"Podcast: Indra Sahdan on grooming Singapore's next footballing goal machine\", 'Get healthy living tips for 2021', \"Follow ST's green podcasters in 2021\", 'MORE', 'Today in Pictures, Jan 13, 2021', 'Rain, rain, go away', 'MORE', \"Malaysia's state of emergency: What you need to know\", \"Malaysian PM Muhyiddin's government has collapsed, says Umno MP after withdrawing support\", 'Is marriage of HK celeb couple Vincent Wong and Yoyo Chen on the rocks?', \"Lucasfilm to sell Sandcrawler building in S'pore for $175m to US firm\", 'Trump becomes first US president to be impeached twice', 'Malaysian government has adequate powers, does not need to declare emergency: Mahathir', \"Indonesia's Sriwijaya flew old planes and neglected routes to become number 3 carrier\", 'Nancy of K-pop group Momoland secretly filmed changing backstage by staff, prompting legal action', 'Malaysian PM Muhyiddin sharply criticised for emergency declaration', \"Fish and vegetable prices up at wet markets in S'pore due to floods in Malaysia\", 'Available for iPhones and iPads', 'Available in Google Play', 'Back to the top', '', '', 'E-paper', 'Facebook', 'Instagram', 'Twitter', 'Podcasts', 'RSS Feed', 'Telegram', 'Youtube', 'Singapore', 'Asia', 'World', 'Opinion', 'Life', 'Business', 'Tech', 'Sport', 'Videos', 'Podcasts', 'Multimedia', 'Terms & Conditions', 'Data Protection Policy', 'Need help? Reach us here.', 'Advertise with us', 'Sign up', 'newsletters']\n"
     ]
    }
   ],
   "source": [
    "a_list = []            \n",
    "for b in other_tag1[0:]:\n",
    "    result = b.text.strip()\n",
    "    a_list.append(result)\n",
    "print(a_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Saving the extracted information into DataFrame\n",
    "Using \"a-list\" above as example, and convert it into a Dataframe. As a dataframe can be save into another format accordingly, for example the CSV, Excel and etc.\n",
    "\n",
    "Cell below showing the conversion into dataframe by using Pandas.DataFrame() function, and the \"a_list\" being saved into a \"df\" with column name \"Tag-a\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag-a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skip to main content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Straits Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Straits Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>International</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>E-paper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Singapore</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Tag-a\n",
       "0  Skip to main content\n",
       "1     The Straits Times\n",
       "2     The Straits Times\n",
       "3         International\n",
       "4             Singapore\n",
       "5               E-paper\n",
       "6                  Home\n",
       "7             Singapore"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "df['Tag-a'] = a_list\n",
    "\n",
    "df.head(8)         # display the first 8 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *** Adding a Complete Program\n",
    "By putting all the function above in a complete program, it will display all the result once in text format. The \"tag_a\" and \"tag_span\" will be saved in a list accordingly and convert into DataFrame in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Header and Sub-header in this website :  14\n",
      "The Straits Times\n",
      "\n",
      "            Top Stories          \n",
      "\n",
      "            top picks          \n",
      "\n",
      "            covid-19          \n",
      "\n",
      "            For Subscribers          \n",
      "\n",
      "            VIEWS          \n",
      "\n",
      "            Asian Insider          \n",
      "\n",
      "            DISCOVER          \n",
      "\n",
      "            Videos          \n",
      "\n",
      "            GZERO MEDIA          \n",
      "\n",
      "            PODCASTS          \n",
      "\n",
      "            MULTIMEDIA          \n",
      "\n",
      "            MOST POPULAR          \n",
      "\n",
      "            Branded Content          \n",
      "\n",
      "Total 'a'-tag in this website (displaying 8 rows):  209\n",
      "Skip to main content\n",
      "The Straits Times\n",
      "The Straits Times\n",
      "International\n",
      "Singapore\n",
      "E-paper\n",
      "Home\n",
      "Singapore\n",
      "['Skip to main content', 'The Straits Times', 'The Straits Times', 'International', 'Singapore', 'E-paper', 'Home', 'Singapore']\n",
      "\n",
      "Total SPAN tag in this website (displaying 8 rows):  97\n",
      "Toggle navigation\n",
      "\n",
      "\n",
      "\n",
      "The Straits Times\n",
      "The Straits Times\n",
      "Toggle navigation\n",
      "\n",
      "['Toggle navigation', '', '', '', 'The Straits Times', 'The Straits Times', 'Toggle navigation', '']\n",
      "\n",
      "Links:  209\n",
      "Images:  261\n"
     ]
    }
   ],
   "source": [
    "# Import Libreries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Assigning website link to \"url\".\n",
    "# url = input(\"Please Enter the website address: \")     # Option for user to input a link.\n",
    "url = \"https://www.straitstimes.com/global\"\n",
    "connection2 = requests.get(url, headers = {\"user-agent\" : \"Mozilla/80.0\"})  # Requests\n",
    "\n",
    "# Connection status check\n",
    "connection2.status_code  \n",
    "    \n",
    "# Applying BeautifulSoup\n",
    "soup = BeautifulSoup(connection2.content, 'html.parser') \n",
    "# print(soup)\n",
    "\n",
    "# Extracting the Header (h1) and Sub-header (h2) with for-loop.\n",
    "many_link1=soup.find_all(['h1','h2'])\n",
    "total_links=len(many_link1) \n",
    "print(\"\\nTotal Header and Sub-header in this website : \", total_links)\n",
    "\n",
    "for a in many_link1:          # print both h1 and h2 in text\n",
    "    print(a.get_text())\n",
    "\n",
    "\n",
    "# Create a list \"tag-a\" to further saving the retrieved information into Dataframe.\n",
    "tag_a = []\n",
    "many_link2=soup.find_all('a', href=True)\n",
    "total_links=len(many_link2) \n",
    "print(\"\\nTotal 'a'-tag in this website (displaying 8 rows): \", total_links)\n",
    "\n",
    "for b in many_link2[:8]:         # print the first 8 rows of \"a\" tag in text\n",
    "    print(b.get_text())\n",
    "    tag_a.append(b.get_text())   # Adding the information into \"list\".\n",
    "print(tag_a)    \n",
    "    \n",
    "    \n",
    "tag_span =[]   \n",
    "many_link3=soup.find_all('span')\n",
    "total_links=len(many_link3) \n",
    "print(\"\\nTotal SPAN tag in this website (displaying 8 rows): \", total_links)\n",
    "\n",
    "for c in many_link3[:8]:        # print the first 8 rows of \"span\" tag in text\n",
    "    print(c.get_text())\n",
    "    tag_span.append(c.get_text())\n",
    "print(tag_span)  \n",
    "\n",
    "\n",
    "# Additional check in counting the \"link\" and \"image\" on the web-page.\n",
    "# Counting number of links\n",
    "count = 0\n",
    "for link in soup.find_all('a', href=True):\n",
    "    count = count+1\n",
    "print(\"\\nLinks: \", count)\n",
    "\n",
    "# Counting image\n",
    "for img in soup.findAll():\n",
    "    if(img.name == 'img'):\n",
    "        count = count+1\n",
    "print(\"Images: \", count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Svaing the Information into a DataFrame\n",
    "Create a Dataframe (df1) that having column Tag-a and Tag-Span wiht the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag-a</th>\n",
       "      <th>Tag-Span</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skip to main content</td>\n",
       "      <td>Toggle navigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Straits Times</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Straits Times</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>International</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Singapore</td>\n",
       "      <td>The Straits Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>E-paper</td>\n",
       "      <td>The Straits Times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Home</td>\n",
       "      <td>Toggle navigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Singapore</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Tag-a           Tag-Span\n",
       "0  Skip to main content  Toggle navigation\n",
       "1     The Straits Times                   \n",
       "2     The Straits Times                   \n",
       "3         International                   \n",
       "4             Singapore  The Straits Times\n",
       "5               E-paper  The Straits Times\n",
       "6                  Home  Toggle navigation\n",
       "7             Singapore                   "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.DataFrame({'Tag-a': tag_a,'Tag-Span': tag_span})     # Create Dataframe\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
